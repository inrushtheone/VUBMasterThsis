{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In this notebook, statistical analysis methods will be used to investigate the relationship between citation number and topics. More specifically, it is envised to perform research on if topic can be used to predict the citation. \n",
    "As always, the first step is to set up the environment and load functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the environment \n",
    "# Read the content of the setup script\n",
    "setup_script = 'Setup/step4.py'\n",
    "\n",
    "with open(setup_script, 'r') as file:\n",
    "    setup_code = file.read()\n",
    "\n",
    "# Execute the setup script\n",
    "exec(setup_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each paper is assigned a dominant topic using either Latent Dirichlet Allocation (LDA) or Non-Negative Matrix Factorization (NMF). These dominant topics are then used to categorize the papers into 20 distinct groups. To investigate the differences in mean citation counts across these groups, we will perform a statistical test. The hypotheses for this test are as follows:\n",
    "\n",
    "- **Null Hypothesis (H0)**: The mean citation count is the same for all groups.\n",
    "- **Alternative Hypothesis (H1)**: The mean citation count differs among the groups.\n",
    "\n",
    "We will use Analysis of Variance (ANOVA) to conduct this test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         sum_sq      df         F    PR(>F)\n",
      "C(dominant_topic)  4.432083e+05    19.0  1.147219  0.294769\n",
      "Residual           1.793804e+08  8822.0       NaN       NaN\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file\n",
    "file_path = 'ProcessedData/lda_for_regression.csv'  # Replace with your actual file path\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Perform ANOVA\n",
    "model = ols('n_citation ~ C(dominant_topic)', data=data).fit()\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "# Print the ANOVA table\n",
    "print(anova_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After performing the ANOVA test on the dataset, we obtained the following results:\n",
    "\n",
    "- The sum of squares (sum_sq) for the factor \"C(dominant_topic)\" is 4.432083e+05.\n",
    "- The degrees of freedom (df) for the factor \"C(dominant_topic)\" is 19.0.\n",
    "- The F-statistic (F) for the ANOVA test is 1.147219.\n",
    "- The p-value (PR(>F)) for the ANOVA test is 0.294769.\n",
    "\n",
    "These results indicate that there is no significant difference in the mean citation count among the different groups defined by the dominant topic.\n",
    "\n",
    "Please note that the ANOVA test assumes the null hypothesis that the mean citation count is the same for all groups. The obtained p-value of 0.294769 suggests that we fail to reject the null hypothesis, indicating that there is no evidence to support a significant difference in the mean citation count among the groups.\n",
    "\n",
    "However, one of the assumptions for ANOVA is that the data follows a normal distribution, which might not be the case here. The next step is to check the normality of the data. This can be done using normality tests such as the Shapiro-Wilk test, Q-Q plots, or the Kolmogorov-Smirnov test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapiro-Wilk Test:\n",
      "Statistic: 0.13504594564437866, p-value: 0.0\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import shapiro\n",
    "\n",
    "# Perform Shapiro-Wilk test on the n_citation column\n",
    "stat, p = shapiro(data['n_citation'])\n",
    "\n",
    "# Print the results\n",
    "print('Shapiro-Wilk Test:')\n",
    "print(f'Statistic: {stat}, p-value: {p}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Shapiro-Wilk test was used to assess the normality of the citation data. The test returned a very low statistic (0.135) and a p-value of 0.0, indicating that the citation data significantly deviates from a normal distribution.\n",
    "\n",
    "Given this result, the normality assumption for ANOVA is violated. Therefore, we will consider the following two alternatives:\n",
    "\n",
    "1. **Data Transformation**: Apply transformations to the data to achieve normality.\n",
    "2. **Non-parametric Test**: Use a non-parametric test that does not assume normality, such as the Kruskal-Wallis test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         sum_sq      df         F    PR(>F)\n",
      "C(dominant_topic)    128.127529    19.0  3.075667  0.000007\n",
      "Residual           19342.680589  8822.0       NaN       NaN\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "\n",
    "\n",
    "# Apply a logarithmic transformation to the n_citation column\n",
    "data['log_n_citation'] = np.log(data['n_citation'] + 1)\n",
    "\n",
    "# Perform ANOVA on transformed data\n",
    "model_log = ols('log_n_citation ~ C(dominant_topic)', data=data).fit()\n",
    "anova_table_log = sm.stats.anova_lm(model_log, typ=2)\n",
    "\n",
    "# Print the ANOVA table\n",
    "print(anova_table_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kruskal-Wallis H Test:\n",
      "Statistic: 59.060234952429, p-value: 5.444105286519811e-06\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import kruskal\n",
    "\n",
    "# Perform Kruskal-Wallis H Test\n",
    "groups = data.groupby('dominant_topic')['n_citation'].apply(list)\n",
    "stat, p = kruskal(*groups)\n",
    "\n",
    "# Print the results\n",
    "print('Kruskal-Wallis H Test:')\n",
    "print(f'Statistic: {stat}, p-value: {p}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformation and ANOVA Results\n",
    "\n",
    "To address the normality issue, we applied a logarithmic transformation to the citation data and then performed an ANOVA test. The results are as follows:\n",
    "\n",
    "- **Sum of Squares (C(dominant_topic))**: 128.127529\n",
    "- **Degrees of Freedom (C(dominant_topic))**: 19\n",
    "- **F-statistic**: 3.075667\n",
    "- **p-value**: 0.000007\n",
    "\n",
    "The very low p-value (< 0.05) indicates a statistically significant difference in the mean citation counts among the different dominant topic groups.\n",
    "\n",
    "### Kruskal-Wallis H Test Results\n",
    "\n",
    "Given the initial non-normal distribution of the citation data, we also performed a Kruskal-Wallis H test, a non-parametric alternative to ANOVA. The results are as follows:\n",
    "\n",
    "- **Kruskal-Wallis H Statistic**: 59.060234952429\n",
    "- **p-value**: 5.444105286519811e-06\n",
    "\n",
    "The extremely low p-value (< 0.05) from the Kruskal-Wallis test further confirms a significant difference in citation counts among the different dominant topic groups.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Both the ANOVA test on the log-transformed data and the Kruskal-Wallis H test on the original data provide strong evidence that the mean citation counts differ significantly across the 20 groups categorized by dominant topics.\n",
    "\n",
    "- The ANOVA test indicates significant differences with an F-statistic of 3.075667 and a p-value of 0.000007.\n",
    "- The Kruskal-Wallis H test supports this finding with a test statistic of 59.060234952429 and a p-value of 5.444105286519811e-06.\n",
    "\n",
    "These results suggest that dominant topics play a significant role in the citation counts of the papers, and the differences in citations among these groups are statistically significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having established that the mean citation counts among these groups are statistically significant, the next step is to use regression techniques for potential prediction. By applying regression analysis, we can model the relationship between dominant topics and citation counts, allowing us to predict citation counts based on the identified topics.\n",
    "\n",
    "### Single Linear Regression Model\n",
    "\n",
    "We begin with a single linear regression model where the number of citations serves as the dependent variable, and the topic loadings are the independent variables. This approach helps us understand how variations in the presence and strength of specific topics influence the number of citations an article receives.\n",
    "\n",
    "The single linear regression model can be represented as:\n",
    "\n",
    "ð‘Œ=\n",
    "ð›½\n",
    "0\n",
    "+\n",
    "ð›½\n",
    "1\n",
    "ð‘‹\n",
    "1\n",
    "+\n",
    "ðœ–\n",
    "\n",
    "Where:\n",
    "- ð‘Œ is the dependent variable representing the number of citations.\n",
    "- ð›½0 is the intercept.\n",
    "- ð›½1 is the coefficient for the independent variable ð‘‹1, which represents the topic loadings.\n",
    "- ðœ– is the error term.\n",
    "\n",
    "By fitting this model to our data, we aim to estimate the coefficients ð›½0 and ð›½1, which will provide insights into the strength and direction of the relationship between dominant topics and citation counts. \n",
    "\n",
    "### Model Evaluation\n",
    "\n",
    "To assess the goodness-of-fit of our model, we will use metrics such as the R-squared value. The R-squared value indicates how well our independent variable (topic loadings) explains the variability in the dependent variable (citation counts). A higher R-squared value implies a better fit of the model to the data.\n",
    "\n",
    "### Future Steps\n",
    "\n",
    "This initial application of single linear regression sets the foundation for more complex models, such as multiple linear regression, which can incorporate multiple independent variables (various topic loadings) simultaneously. By progressively refining our models, we aim to enhance the accuracy and predictive power of our analysis, ultimately contributing to a deeper understanding of the factors driving citation counts in academic literature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:             n_citation   R-squared:                       0.003\n",
      "Model:                            OLS   Adj. R-squared:                  0.000\n",
      "Method:                 Least Squares   F-statistic:                     1.115\n",
      "Date:                Sat, 27 Jul 2024   Prob (F-statistic):              0.327\n",
      "Time:                        18:59:57   Log-Likelihood:                -45630.\n",
      "No. Observations:                7073   AIC:                         9.130e+04\n",
      "Df Residuals:                    7053   BIC:                         9.144e+04\n",
      "Df Model:                          19                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         34.0711      5.039      6.761      0.000      24.193      43.949\n",
      "1             -8.0526     21.489     -0.375      0.708     -50.178      34.073\n",
      "2             -6.1581      6.997     -0.880      0.379     -19.874       7.558\n",
      "3             22.6774      9.795      2.315      0.021       3.476      41.879\n",
      "4            -20.8125     15.118     -1.377      0.169     -50.447       8.822\n",
      "5             -1.1035     10.991     -0.100      0.920     -22.649      20.442\n",
      "6             -3.6425     11.731     -0.311      0.756     -26.638      19.353\n",
      "7              5.8664      7.947      0.738      0.460      -9.713      21.445\n",
      "8             -7.5635      7.615     -0.993      0.321     -22.491       7.364\n",
      "9            -22.9750     15.874     -1.447      0.148     -54.093       8.143\n",
      "10            -8.4241     26.805     -0.314      0.753     -60.969      44.121\n",
      "11             1.9599     14.425      0.136      0.892     -26.317      30.236\n",
      "12           -13.4085     13.037     -1.028      0.304     -38.965      12.148\n",
      "13            -6.9674      7.799     -0.893      0.372     -22.256       8.322\n",
      "14            -5.1170      8.284     -0.618      0.537     -21.357      11.123\n",
      "15             2.8047     13.708      0.205      0.838     -24.067      29.677\n",
      "16           -16.0578     18.428     -0.871      0.384     -52.182      20.067\n",
      "17            -3.7484     10.921     -0.343      0.731     -25.158      17.661\n",
      "18           -11.9595      9.158     -1.306      0.192     -29.912       5.993\n",
      "19           -13.5696      9.917     -1.368      0.171     -33.010       5.871\n",
      "==============================================================================\n",
      "Omnibus:                    21105.238   Durbin-Watson:                   2.015\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):       1953319770.904\n",
      "Skew:                          42.697   Prob(JB):                         0.00\n",
      "Kurtosis:                    2576.069   Cond. No.                         15.9\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "Mean Squared Error: 7478.543528442862\n",
      "R-squared: -0.008430529755289129\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Assuming 'data' is your DataFrame\n",
    "# Define the independent variables and dependent variable\n",
    "X = pd.get_dummies(data['dominant_topic'], drop_first=True)\n",
    "y = data['n_citation']\n",
    "\n",
    "# Split the data into training and testing sets using the index\n",
    "train_indices, test_indices = train_test_split(data.index, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create training and testing sets\n",
    "X_train, X_test = X.loc[train_indices], X.loc[test_indices]\n",
    "y_train, y_test = y.loc[train_indices], y.loc[test_indices]\n",
    "\n",
    "# Add a constant term to the independent variables\n",
    "X_train = sm.add_constant(X_train)\n",
    "X_test = sm.add_constant(X_test)\n",
    "\n",
    "# Fit the linear regression model on the training set\n",
    "model = sm.OLS(y_train, X_train).fit()\n",
    "\n",
    "# Print the model summary\n",
    "print(model.summary())\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'R-squared: {r2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The OLS regression model was fitted to predict the number of citations (`n_citation`). The R-squared value is very low at 0.002, indicating that the model explains only 0.2% of the variance in the dependent variable. The F-statistic is 1.147 with a p-value of 0.295, suggesting that the overall model is not statistically significant.\n",
    "\n",
    "### Key Coefficients\n",
    "- **Intercept**: 35.1896 (p < 0.001), representing the baseline number of citations.\n",
    "- **Variable 3**: 16.5802 (p = 0.044), one of the few statistically significant predictors.\n",
    "\n",
    "### Summary\n",
    "Overall, the model demonstrates poor explanatory power, as many predictors do not significantly contribute to explaining the variation in citation counts.\n",
    "\n",
    "## Regression with all topic loadings. \n",
    "Next, we perform the regression using all topic loadings instead of only the domiant topic. \n",
    "The result is still not satisfisfactory. We know already from preivous steps that the citation is not normal distributed. The linear relationship might not exisit. As such, we will furhter perfrom random foreast and deep learning technicals which could potentially take into account any non-linear relationships. Given the size of the avaiable data, they could be better alternatives. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:             n_citation   R-squared:                       0.003\n",
      "Model:                            OLS   Adj. R-squared:                 -0.000\n",
      "Method:                 Least Squares   F-statistic:                    0.9376\n",
      "Date:                Sat, 27 Jul 2024   Prob (F-statistic):              0.538\n",
      "Time:                        19:01:14   Log-Likelihood:                -45631.\n",
      "No. Observations:                7073   AIC:                         9.130e+04\n",
      "Df Residuals:                    7052   BIC:                         9.145e+04\n",
      "Df Model:                          20                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const       -227.6425    217.023     -1.049      0.294    -653.073     197.788\n",
      "topic0       267.3335    219.840      1.216      0.224    -163.618     698.285\n",
      "topic1       230.0927    222.441      1.034      0.301    -205.958     666.144\n",
      "topic2       257.9121    220.646      1.169      0.242    -174.621     690.445\n",
      "topic3       282.2976    219.877      1.284      0.199    -148.728     713.323\n",
      "topic4       247.0184    219.996      1.123      0.262    -184.241     678.278\n",
      "topic5       276.8676    220.065      1.258      0.208    -154.525     708.261\n",
      "topic6       278.7047    220.868      1.262      0.207    -154.263     711.673\n",
      "topic7       275.9913    219.940      1.255      0.210    -155.157     707.140\n",
      "topic8       256.6931    219.630      1.169      0.243    -173.848     687.235\n",
      "topic9       223.8066    221.691      1.010      0.313    -210.775     658.388\n",
      "topic10      246.1444    223.065      1.103      0.270    -191.130     683.419\n",
      "topic11      268.8734    220.056      1.222      0.222    -162.503     700.249\n",
      "topic12      245.6567    220.305      1.115      0.265    -186.208     677.521\n",
      "topic13      262.1259    219.358      1.195      0.232    -167.881     692.133\n",
      "topic14      250.5075    220.490      1.136      0.256    -181.718     682.733\n",
      "topic15      271.9076    219.877      1.237      0.216    -159.117     702.933\n",
      "topic16      253.7465    222.420      1.141      0.254    -182.264     689.757\n",
      "topic17      277.5918    219.636      1.264      0.206    -152.960     708.144\n",
      "topic18      239.2168    219.929      1.088      0.277    -191.911     670.344\n",
      "topic19      249.3291    219.350      1.137      0.256    -180.662     679.321\n",
      "==============================================================================\n",
      "Omnibus:                    21122.424   Durbin-Watson:                   2.014\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):       1965020035.471\n",
      "Skew:                          42.793   Prob(JB):                         0.00\n",
      "Kurtosis:                    2583.766   Cond. No.                         570.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "Mean Squared Error: 7371.662113516692\n",
      "R-squared: 0.0059816992389083445\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Assuming 'data' is your DataFrame\n",
    "# Define the dependent variable\n",
    "y = data['n_citation']\n",
    "\n",
    "# Define the independent variables\n",
    "X = data[['topic0', 'topic1', 'topic2', 'topic3', 'topic4', 'topic5', 'topic6', 'topic7', 'topic8', 'topic9', 'topic10', 'topic11', 'topic12', 'topic13', 'topic14', 'topic15', 'topic16', 'topic17', 'topic18', 'topic19']]\n",
    "\n",
    "\n",
    "\n",
    "# Create training and testing sets\n",
    "X_train, X_test = X.loc[train_indices], X.loc[test_indices]\n",
    "y_train, y_test = y.loc[train_indices], y.loc[test_indices]\n",
    "\n",
    "# Add a constant term to the independent variables\n",
    "X_train = sm.add_constant(X_train)\n",
    "X_test = sm.add_constant(X_test)\n",
    "\n",
    "# Fit the linear regression model on the training set\n",
    "model = sm.OLS(y_train, X_train).fit()\n",
    "\n",
    "# Print the model summary\n",
    "print(model.summary())\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'R-squared: {r2}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 8105.646896608254\n",
      "R-squared: -0.09299113695978112\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Assuming 'data' is your DataFrame\n",
    "# Define the dependent variable\n",
    "y = data['n_citation']\n",
    "\n",
    "# Define the independent variables\n",
    "X = data[['topic0', 'topic1', 'topic2', 'topic3', 'topic4', 'topic5', 'topic6', 'topic7', 'topic8', 'topic9', 'topic10', 'topic11', 'topic12', 'topic13', 'topic14', 'topic15', 'topic16', 'topic17', 'topic18', 'topic19']]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test = X.loc[train_indices], X.loc[test_indices]\n",
    "y_train, y_test = y.loc[train_indices], y.loc[test_indices]\n",
    "\n",
    "# Initialize the Random Forest Regressor\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-90-3de147d34399>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Define the dependent variable\n",
    "y = data['n_citation']\n",
    "\n",
    "# Define the independent variables\n",
    "X = data[['topic0', 'topic1', 'topic2', 'topic3', 'topic4', 'topic5', 'topic6', 'topic7', 'topic8', 'topic9', 'topic10', 'topic11', 'topic12', 'topic13', 'topic14', 'topic15', 'topic16', 'topic17', 'topic18', 'topic19']]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test = X.loc[train_indices], X.loc[test_indices]\n",
    "y_train, y_test = y.loc[train_indices], y.loc[test_indices]\n",
    "\n",
    "\n",
    "# Normalize the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Build the neural network model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=100, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test).flatten()\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared: {r2}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
