{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In this notebook, statistical analysis methods will be used to investigate the relationship between citation number and topics. More specifically, it is envised to perform research on if topic can be used to predict the citation. \n",
    "As always, the first step is to set up the environment and load functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the environment \n",
    "# Read the content of the setup script\n",
    "setup_script = 'Setup/step4.py'\n",
    "\n",
    "with open(setup_script, 'r') as file:\n",
    "    setup_code = file.read()\n",
    "\n",
    "# Execute the setup script\n",
    "exec(setup_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each paper is assigned a dominant topic using either Latent Dirichlet Allocation (LDA) or Non-Negative Matrix Factorization (NMF). These dominant topics are then used to categorize the papers into 20 distinct groups. To investigate the differences in mean citation counts across these groups, we will perform a statistical test. The hypotheses for this test are as follows:\n",
    "\n",
    "- **Null Hypothesis (H0)**: The mean citation count is the same for all groups.\n",
    "- **Alternative Hypothesis (H1)**: The mean citation count differs among the groups.\n",
    "\n",
    "We will use Analysis of Variance (ANOVA) to conduct this test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         sum_sq      df         F    PR(>F)\n",
      "C(dominant_topic)  4.432083e+05    19.0  1.147219  0.294769\n",
      "Residual           1.793804e+08  8822.0       NaN       NaN\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file\n",
    "file_path = 'ProcessedData/lda_for_regression.csv'  # Replace with your actual file path\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Perform ANOVA\n",
    "model = ols('n_citation ~ C(dominant_topic)', data=data).fit()\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "# Print the ANOVA table\n",
    "print(anova_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After performing the ANOVA test on the dataset, we obtained the following results:\n",
    "\n",
    "- The sum of squares (sum_sq) for the factor \"C(dominant_topic)\" is 4.432083e+05.\n",
    "- The degrees of freedom (df) for the factor \"C(dominant_topic)\" is 19.0.\n",
    "- The F-statistic (F) for the ANOVA test is 1.147219.\n",
    "- The p-value (PR(>F)) for the ANOVA test is 0.294769.\n",
    "\n",
    "These results indicate that there is no significant difference in the mean citation count among the different groups defined by the dominant topic.\n",
    "\n",
    "Please note that the ANOVA test assumes the null hypothesis that the mean citation count is the same for all groups. The obtained p-value of 0.294769 suggests that we fail to reject the null hypothesis, indicating that there is no evidence to support a significant difference in the mean citation count among the groups.\n",
    "\n",
    "However, one of the assumptions for ANOVA is that the data follows a normal distribution, which might not be the case here. The next step is to check the normality of the data. This can be done using normality tests such as the Shapiro-Wilk test, Q-Q plots, or the Kolmogorov-Smirnov test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapiro-Wilk Test:\n",
      "Statistic: 0.14591550827026367, p-value: 0.0\n",
      "Shapiro-Wilk Test:\n",
      "Statistic: 0.13504594564437866, p-value: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\feng\\.conda\\envs\\mygensim\\lib\\site-packages\\scipy\\stats\\morestats.py:1681: UserWarning: p-value may not be accurate for N > 5000.\n",
      "  warnings.warn(\"p-value may not be accurate for N > 5000.\")\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import shapiro\n",
    "\n",
    "# Perform Shapiro-Wilk test on the n_citation column\n",
    "stat, p = shapiro(data['n_citation'])\n",
    "\n",
    "# Print the results\n",
    "print('Shapiro-Wilk Test:')\n",
    "print(f'Statistic: {stat}, p-value: {p}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Shapiro-Wilk test was used to assess the normality of the citation data. The test returned a very low statistic (0.135) and a p-value of 0.0, indicating that the citation data significantly deviates from a normal distribution.\n",
    "\n",
    "Given this result, the normality assumption for ANOVA is violated. Therefore, we will consider the following two alternatives:\n",
    "\n",
    "1. **Data Transformation**: Apply transformations to the data to achieve normality.\n",
    "2. **Non-parametric Test**: Use a non-parametric test that does not assume normality, such as the Kruskal-Wallis test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         sum_sq      df         F    PR(>F)\n",
      "C(dominant_topic)    128.127529    19.0  3.075667  0.000007\n",
      "Residual           19342.680589  8822.0       NaN       NaN\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "\n",
    "\n",
    "# Apply a logarithmic transformation to the n_citation column\n",
    "data['log_n_citation'] = np.log(data['n_citation'] + 1)\n",
    "\n",
    "# Perform ANOVA on transformed data\n",
    "model_log = ols('log_n_citation ~ C(dominant_topic)', data=data).fit()\n",
    "anova_table_log = sm.stats.anova_lm(model_log, typ=2)\n",
    "\n",
    "# Print the ANOVA table\n",
    "print(anova_table_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kruskal-Wallis H Test:\n",
      "Statistic: 59.060234952429, p-value: 5.444105286519811e-06\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import kruskal\n",
    "\n",
    "# Perform Kruskal-Wallis H Test\n",
    "groups = data.groupby('dominant_topic')['n_citation'].apply(list)\n",
    "stat, p = kruskal(*groups)\n",
    "\n",
    "# Print the results\n",
    "print('Kruskal-Wallis H Test:')\n",
    "print(f'Statistic: {stat}, p-value: {p}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformation and ANOVA Results\n",
    "\n",
    "To address the normality issue, we applied a logarithmic transformation to the citation data and then performed an ANOVA test. The results are as follows:\n",
    "\n",
    "- **Sum of Squares (C(dominant_topic))**: 128.127529\n",
    "- **Degrees of Freedom (C(dominant_topic))**: 19\n",
    "- **F-statistic**: 3.075667\n",
    "- **p-value**: 0.000007\n",
    "\n",
    "The very low p-value (< 0.05) indicates a statistically significant difference in the mean citation counts among the different dominant topic groups.\n",
    "\n",
    "### Kruskal-Wallis H Test Results\n",
    "\n",
    "Given the initial non-normal distribution of the citation data, we also performed a Kruskal-Wallis H test, a non-parametric alternative to ANOVA. The results are as follows:\n",
    "\n",
    "- **Kruskal-Wallis H Statistic**: 59.060234952429\n",
    "- **p-value**: 5.444105286519811e-06\n",
    "\n",
    "The extremely low p-value (< 0.05) from the Kruskal-Wallis test further confirms a significant difference in citation counts among the different dominant topic groups.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Both the ANOVA test on the log-transformed data and the Kruskal-Wallis H test on the original data provide strong evidence that the mean citation counts differ significantly across the 20 groups categorized by dominant topics.\n",
    "\n",
    "- The ANOVA test indicates significant differences with an F-statistic of 3.075667 and a p-value of 0.000007.\n",
    "- The Kruskal-Wallis H test supports this finding with a test statistic of 59.060234952429 and a p-value of 5.444105286519811e-06.\n",
    "\n",
    "These results suggest that dominant topics play a significant role in the citation counts of the papers, and the differences in citations among these groups are statistically significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having established that the mean citation counts among these groups are statistically significant, the next step is to use regression techniques for potential prediction. By applying regression analysis, we can model the relationship between dominant topics and citation counts, allowing us to predict citation counts based on the identified topics.\n",
    "\n",
    "### Single Linear Regression Model\n",
    "\n",
    "We begin with a single linear regression model where the number of citations serves as the dependent variable, and the topic loadings are the independent variables. This approach helps us understand how variations in the presence and strength of specific topics influence the number of citations an article receives.\n",
    "\n",
    "The single linear regression model can be represented as:\n",
    "\n",
    "ð‘Œ=\n",
    "ð›½\n",
    "0\n",
    "+\n",
    "ð›½\n",
    "1\n",
    "ð‘‹\n",
    "1\n",
    "+\n",
    "ðœ–\n",
    "\n",
    "Where:\n",
    "- ð‘Œ is the dependent variable representing the number of citations.\n",
    "- ð›½0 is the intercept.\n",
    "- ð›½1 is the coefficient for the independent variable ð‘‹1, which represents the topic loadings.\n",
    "- ðœ– is the error term.\n",
    "\n",
    "By fitting this model to our data, we aim to estimate the coefficients ð›½0 and ð›½1, which will provide insights into the strength and direction of the relationship between dominant topics and citation counts. A significant positive coefficient would indicate that higher loadings on certain topics are associated with an increase in citation counts, while a significant negative coefficient would suggest the opposite.\n",
    "\n",
    "### Model Evaluation\n",
    "\n",
    "To assess the goodness-of-fit of our model, we will use metrics such as the R-squared value. The R-squared value indicates how well our independent variable (topic loadings) explains the variability in the dependent variable (citation counts). A higher R-squared value implies a better fit of the model to the data.\n",
    "\n",
    "### Future Steps\n",
    "\n",
    "This initial application of single linear regression sets the foundation for more complex models, such as multiple linear regression, which can incorporate multiple independent variables (various topic loadings) simultaneously. By progressively refining our models, we aim to enhance the accuracy and predictive power of our analysis, ultimately contributing to a deeper understanding of the factors driving citation counts in academic literature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:             n_citation   R-squared:                       0.003\n",
      "Model:                            OLS   Adj. R-squared:                  0.001\n",
      "Method:                 Least Squares   F-statistic:                     1.352\n",
      "Date:                Sat, 27 Jul 2024   Prob (F-statistic):              0.135\n",
      "Time:                        10:13:23   Log-Likelihood:                -56390.\n",
      "No. Observations:                8842   AIC:                         1.128e+05\n",
      "Df Residuals:                    8821   BIC:                         1.130e+05\n",
      "Df Model:                          20                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const       -326.9889    177.735     -1.840      0.066    -675.391      21.413\n",
      "topic0       368.9590    180.004      2.050      0.040      16.110     721.808\n",
      "topic1       341.6521    182.410      1.873      0.061     -15.915     699.219\n",
      "topic2       360.3443    180.699      1.994      0.046       6.132     714.556\n",
      "topic3       378.4365    180.140      2.101      0.036      25.321     731.552\n",
      "topic4       347.1686    180.230      1.926      0.054      -6.125     700.462\n",
      "topic5       374.2021    180.161      2.077      0.038      21.045     727.359\n",
      "topic6       376.0476    180.770      2.080      0.038      21.696     730.399\n",
      "topic7       373.7331    180.059      2.076      0.038      20.776     726.691\n",
      "topic8       359.2522    179.886      1.997      0.046       6.634     711.870\n",
      "topic9       323.8191    181.571      1.783      0.075     -32.103     679.741\n",
      "topic10      344.2752    182.686      1.885      0.060     -13.831     702.382\n",
      "topic11      365.2646    180.574      2.023      0.043      11.298     719.231\n",
      "topic12      342.7143    180.470      1.899      0.058     -11.049     696.477\n",
      "topic13      365.8544    179.678      2.036      0.042      13.644     718.065\n",
      "topic14      349.8088    180.525      1.938      0.053      -4.063     703.681\n",
      "topic15      368.8613    180.120      2.048      0.041      15.785     721.938\n",
      "topic16      356.7374    182.115      1.959      0.050      -0.250     713.725\n",
      "topic17      381.6022    179.804      2.122      0.034      29.144     734.061\n",
      "topic18      342.3613    180.225      1.900      0.058     -10.923     695.645\n",
      "topic19      350.6482    179.686      1.951      0.051      -1.578     702.874\n",
      "==============================================================================\n",
      "Omnibus:                    26453.557   Durbin-Watson:                   1.990\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):       2847283648.722\n",
      "Skew:                          43.063   Prob(JB):                         0.00\n",
      "Kurtosis:                    2781.672   Cond. No.                         562.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Define the dependent variable\n",
    "y = data['n_citation']\n",
    "\n",
    "# Define the independent variable\n",
    "X = data[['topic0', 'topic1', 'topic2', 'topic3', 'topic4', 'topic5', 'topic6', 'topic7', 'topic8', 'topic9', 'topic10', 'topic11', 'topic12', 'topic13', 'topic14', 'topic15', 'topic16', 'topic17', 'topic18', 'topic19']]\n",
    "\n",
    "# Add a constant term to the independent variable\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit the linear regression model\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Print the model summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 8105.646896608254\n",
      "R-squared: -0.09299113695978112\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Define the dependent variable\n",
    "y = data['n_citation']\n",
    "\n",
    "# Define the independent variables\n",
    "X = data[['topic0', 'topic1', 'topic2', 'topic3', 'topic4', 'topic5', 'topic6', 'topic7', 'topic8', 'topic9', 'topic10', 'topic11', 'topic12', 'topic13', 'topic14', 'topic15', 'topic16', 'topic17', 'topic18', 'topic19']]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Random Forest Regressor\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-974ce73aeef7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Define the dependent variable\n",
    "y = data['n_citation']\n",
    "\n",
    "# Define the independent variables\n",
    "X = data[['topic0', 'topic1', 'topic2', 'topic3', 'topic4', 'topic5', 'topic6', 'topic7', 'topic8', 'topic9', 'topic10', 'topic11', 'topic12', 'topic13', 'topic14', 'topic15', 'topic16', 'topic17', 'topic18', 'topic19']]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Build the neural network model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=100, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test).flatten()\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.13.0\n",
      "  Downloading tensorflow-2.13.0-cp38-cp38-win_amd64.whl (1.9 kB)\n",
      "Collecting tensorflow-intel==2.13.0; platform_system == \"Windows\"\n",
      "  Downloading tensorflow_intel-2.13.0-cp38-cp38-win_amd64.whl (276.5 MB)\n",
      "Requirement already satisfied: packaging in c:\\users\\feng\\.conda\\envs\\mygensim\\lib\\site-packages (from tensorflow-intel==2.13.0; platform_system == \"Windows\"->tensorflow==2.13.0) (20.4)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\feng\\.conda\\envs\\mygensim\\lib\\site-packages (from tensorflow-intel==2.13.0; platform_system == \"Windows\"->tensorflow==2.13.0) (2.10.0)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\feng\\.conda\\envs\\mygensim\\lib\\site-packages (from tensorflow-intel==2.13.0; platform_system == \"Windows\"->tensorflow==2.13.0) (50.3.1.post20201107)\n",
      "Collecting flatbuffers>=23.1.21\n",
      "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting absl-py>=1.0.0\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting tensorboard<2.14,>=2.13\n",
      "  Downloading tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3\n",
      "  Downloading protobuf-4.25.4-cp38-cp38-win_amd64.whl (413 kB)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\feng\\.conda\\envs\\mygensim\\lib\\site-packages (from tensorflow-intel==2.13.0; platform_system == \"Windows\"->tensorflow==2.13.0) (1.11.2)\n",
      "Collecting keras<2.14,>=2.13.1\n",
      "  Downloading keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1; platform_machine != \"arm64\" or platform_system != \"Darwin\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an EnvironmentError: [WinError 5] æ‹’ç»è®¿é—®ã€‚: 'C:\\\\Users\\\\feng\\\\.conda\\\\envs\\\\mygensim\\\\lib\\\\site-packages\\\\~umpy\\\\core\\\\_multiarray_tests.cp38-win_amd64.pyd'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.31.0-cp38-cp38-win_amd64.whl (1.5 MB)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\feng\\.conda\\envs\\mygensim\\lib\\site-packages (from tensorflow-intel==2.13.0; platform_system == \"Windows\"->tensorflow==2.13.0) (1.15.0)\n",
      "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in c:\\users\\feng\\.conda\\envs\\mygensim\\lib\\site-packages (from tensorflow-intel==2.13.0; platform_system == \"Windows\"->tensorflow==2.13.0) (3.7.4.3)\n",
      "Collecting numpy<=1.24.3,>=1.22\n",
      "  Downloading numpy-1.24.3-cp38-cp38-win_amd64.whl (14.9 MB)\n",
      "Collecting tensorflow-estimator<2.14,>=2.13.0\n",
      "  Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.65.1-cp38-cp38-win_amd64.whl (4.1 MB)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\feng\\.conda\\envs\\mygensim\\lib\\site-packages (from packaging->tensorflow-intel==2.13.0; platform_system == \"Windows\"->tensorflow==2.13.0) (2.4.7)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\feng\\.conda\\envs\\mygensim\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.13.0; platform_system == \"Windows\"->tensorflow==2.13.0) (0.35.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\feng\\.conda\\envs\\mygensim\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0; platform_system == \"Windows\"->tensorflow==2.13.0) (1.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\feng\\.conda\\envs\\mygensim\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0; platform_system == \"Windows\"->tensorflow==2.13.0) (2.24.0)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5\n",
      "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\feng\\.conda\\envs\\mygensim\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0; platform_system == \"Windows\"->tensorflow==2.13.0) (1.22.1)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.6-py3-none-any.whl (105 kB)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\feng\\.conda\\envs\\mygensim\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0; platform_system == \"Windows\"->tensorflow==2.13.0) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\feng\\.conda\\envs\\mygensim\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0; platform_system == \"Windows\"->tensorflow==2.13.0) (2022.6.15)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\feng\\.conda\\envs\\mygensim\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0; platform_system == \"Windows\"->tensorflow==2.13.0) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\feng\\.conda\\envs\\mygensim\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0; platform_system == \"Windows\"->tensorflow==2.13.0) (2.10)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\feng\\.conda\\envs\\mygensim\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0; platform_system == \"Windows\"->tensorflow==2.13.0) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\feng\\.conda\\envs\\mygensim\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0; platform_system == \"Windows\"->tensorflow==2.13.0) (4.2.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.5\" in c:\\users\\feng\\.conda\\envs\\mygensim\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0; platform_system == \"Windows\"->tensorflow==2.13.0) (4.7.2)\n",
      "Collecting importlib-metadata>=4.4; python_version < \"3.10\"\n",
      "  Downloading importlib_metadata-8.2.0-py3-none-any.whl (25 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\feng\\.conda\\envs\\mygensim\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0; platform_system == \"Windows\"->tensorflow==2.13.0) (0.4.8)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\feng\\.conda\\envs\\mygensim\\lib\\site-packages (from importlib-metadata>=4.4; python_version < \"3.10\"->markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0; platform_system == \"Windows\"->tensorflow==2.13.0) (3.4.0)\n",
      "Installing collected packages: numpy, opt-einsum, libclang, google-pasta, flatbuffers, absl-py, astunparse, gast, protobuf, grpcio, oauthlib, requests-oauthlib, google-auth-oauthlib, tensorboard-data-server, importlib-metadata, markdown, tensorboard, keras, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorflow-intel, tensorflow\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.2\n",
      "    Uninstalling numpy-1.19.2:\n",
      "      Successfully uninstalled numpy-1.19.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow==2.13.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:             n_citation   R-squared:                       0.002\n",
      "Model:                            OLS   Adj. R-squared:                  0.000\n",
      "Method:                 Least Squares   F-statistic:                     1.147\n",
      "Date:                Sat, 27 Jul 2024   Prob (F-statistic):              0.295\n",
      "Time:                        10:08:42   Log-Likelihood:                -56393.\n",
      "No. Observations:                8842   AIC:                         1.128e+05\n",
      "Df Residuals:                    8822   BIC:                         1.130e+05\n",
      "Df Model:                          19                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         35.1896      4.196      8.387      0.000      26.965      43.414\n",
      "1            -11.6973     18.178     -0.644      0.520     -47.330      23.935\n",
      "2             -5.8609      5.837     -1.004      0.315     -17.302       5.581\n",
      "3             16.5802      8.242      2.012      0.044       0.423      32.737\n",
      "4            -15.3353     12.339     -1.243      0.214     -39.524       8.853\n",
      "5             -2.7980      9.133     -0.306      0.759     -20.700      15.104\n",
      "6             -3.9865      9.850     -0.405      0.686     -23.296      15.323\n",
      "7              1.8416      6.634      0.278      0.781     -11.163      14.846\n",
      "8             -7.0778      6.282     -1.127      0.260     -19.393       5.237\n",
      "9            -23.1824     12.802     -1.811      0.070     -48.277       1.912\n",
      "10            -9.1896     21.005     -0.437      0.662     -50.365      31.985\n",
      "11            -4.0519     11.805     -0.343      0.731     -27.193      19.089\n",
      "12           -15.6717     11.040     -1.420      0.156     -37.312       5.969\n",
      "13            -5.9303      6.491     -0.914      0.361     -18.654       6.793\n",
      "14            -8.1325      6.883     -1.182      0.237     -21.625       5.360\n",
      "15            -1.5174     11.427     -0.133      0.894     -23.916      20.881\n",
      "16           -13.1452     15.605     -0.842      0.400     -43.735      17.445\n",
      "17             0.0979      9.008      0.011      0.991     -17.560      17.756\n",
      "18           -14.3992      7.628     -1.888      0.059     -29.352       0.554\n",
      "19           -13.1246      8.161     -1.608      0.108     -29.122       2.873\n",
      "==============================================================================\n",
      "Omnibus:                    26429.154   Durbin-Watson:                   1.988\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):       2827432765.269\n",
      "Skew:                          42.953   Prob(JB):                         0.00\n",
      "Kurtosis:                    2771.966   Cond. No.                         15.5\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'data' is your DataFrame and 'dominant_topic' is the categorical variable\n",
    "# Convert the categorical variable into dummy/indicator variables\n",
    "X = pd.get_dummies(data['dominant_topic'], drop_first=True)\n",
    "\n",
    "# Define the dependent variable\n",
    "y = data['n_citation']\n",
    "\n",
    "# Add a constant term to the independent variable\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit the linear regression model\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Print the model summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 7492.065223222995\n",
      "R-squared: -0.010253837967437507\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Assuming 'data' is your DataFrame and 'dominant_topic' is the categorical variable\n",
    "# Convert the categorical variable into dummy/indicator variables\n",
    "X = pd.get_dummies(data['dominant_topic'], drop_first=True)\n",
    "\n",
    "# Define the dependent variable\n",
    "y = data['n_citation']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Random Forest Regressor\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 108 candidates, totalling 324 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   15.8s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   53.2s\n",
      "[Parallel(n_jobs=-1)]: Done 324 out of 324 | elapsed:  1.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\n",
      "Mean Squared Error: 7483.502230339643\n",
      "R-squared: -0.00909917684701922\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Assuming 'data' is your DataFrame and 'dominant_topic' is the categorical variable\n",
    "# Convert the categorical variable into dummy/indicator variables\n",
    "X = pd.get_dummies(data['dominant_topic'], drop_first=True)\n",
    "\n",
    "# Define the dependent variable\n",
    "y = data['n_citation']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Random Forest Regressor\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best estimator\n",
    "best_rf = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared: {r2}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
