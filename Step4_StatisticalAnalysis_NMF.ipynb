{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction - NMF\n",
    "In this notebook, statistical analysis methods will be used to investigate the relationship between citation number and topics. More specifically, it is envised to perform research on if topic can be used to predict the citation. \n",
    "As always, the first step is to set up the environment and load functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the environment \n",
    "# Read the content of the setup script\n",
    "setup_script = 'Setup/step4.py'\n",
    "\n",
    "with open(setup_script, 'r') as file:\n",
    "    setup_code = file.read()\n",
    "\n",
    "# Execute the setup script\n",
    "exec(setup_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each paper is assigned a dominant topic using either Latent Dirichlet Allocation (LDA) or Non-Negative Matrix Factorization (NMF). These dominant topics are then used to categorize the papers into 20 distinct groups. To investigate the differences in mean citation counts across these groups, we will perform a statistical test. The hypotheses for this test are as follows:\n",
    "\n",
    "- **Null Hypothesis (H0)**: The mean citation count is the same for all groups.\n",
    "- **Alternative Hypothesis (H1)**: The mean citation count differs among the groups.\n",
    "\n",
    "We will use Analysis of Variance (ANOVA) to conduct this test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         sum_sq      df         F    PR(>F)\n",
      "C(dominant_topic)  7.006843e+05    19.0  1.816288  0.016142\n",
      "Residual           1.791229e+08  8822.0       NaN       NaN\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file\n",
    "file_path = 'ProcessedData/nmf_for_regression.csv'  # Replace with your actual file path\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Perform ANOVA\n",
    "model = ols('n_citation ~ C(dominant_topic)', data=data).fit()\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "# Print the ANOVA table\n",
    "print(anova_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After performing the ANOVA test on the dataset, we obtained the following results:\n",
    "\n",
    "- The sum of squares (sum_sq) for the factor \"C(dominant_topic)\" is 4.432083e+05.\n",
    "- The degrees of freedom (df) for the factor \"C(dominant_topic)\" is 19.0.\n",
    "- The F-statistic (F) for the ANOVA test is 1.147219.\n",
    "- The p-value (PR(>F)) for the ANOVA test is 0.294769.\n",
    "\n",
    "These results indicate that there is no significant difference in the mean citation count among the different groups defined by the dominant topic.\n",
    "\n",
    "Please note that the ANOVA test assumes the null hypothesis that the mean citation count is the same for all groups. The obtained p-value of 0.294769 suggests that we fail to reject the null hypothesis, indicating that there is no evidence to support a significant difference in the mean citation count among the groups.\n",
    "\n",
    "However, one of the assumptions for ANOVA is that the data follows a normal distribution, which might not be the case here. The next step is to check the normality of the data. This can be done using normality tests such as the Shapiro-Wilk test, Q-Q plots, or the Kolmogorov-Smirnov test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapiro-Wilk Test:\n",
      "Statistic: 0.13504594564437866, p-value: 0.0\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import shapiro\n",
    "\n",
    "# Perform Shapiro-Wilk test on the n_citation column\n",
    "stat, p = shapiro(data['n_citation'])\n",
    "\n",
    "# Print the results\n",
    "print('Shapiro-Wilk Test:')\n",
    "print(f'Statistic: {stat}, p-value: {p}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Shapiro-Wilk test was used to assess the normality of the citation data. The test returned a very low statistic (0.135) and a p-value of 0.0, indicating that the citation data significantly deviates from a normal distribution.\n",
    "\n",
    "Given this result, the normality assumption for ANOVA is violated. Therefore, we will consider the following two alternatives:\n",
    "\n",
    "1. **Data Transformation**: Apply transformations to the data to achieve normality.\n",
    "2. **Non-parametric Test**: Use a non-parametric test that does not assume normality, such as the Kruskal-Wallis test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         sum_sq      df         F    PR(>F)\n",
      "C(dominant_topic)     88.434349    19.0  2.118495  0.003081\n",
      "Residual           19382.373770  8822.0       NaN       NaN\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "\n",
    "\n",
    "# Apply a logarithmic transformation to the n_citation column\n",
    "data['log_n_citation'] = np.log(data['n_citation'] + 1)\n",
    "\n",
    "# Perform ANOVA on transformed data\n",
    "model_log = ols('log_n_citation ~ C(dominant_topic)', data=data).fit()\n",
    "anova_table_log = sm.stats.anova_lm(model_log, typ=2)\n",
    "\n",
    "# Print the ANOVA table\n",
    "print(anova_table_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kruskal-Wallis H Test:\n",
      "Statistic: 39.49312591588303, p-value: 0.003811886916578957\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import kruskal\n",
    "\n",
    "# Perform Kruskal-Wallis H Test\n",
    "groups = data.groupby('dominant_topic')['n_citation'].apply(list)\n",
    "stat, p = kruskal(*groups)\n",
    "\n",
    "# Print the results\n",
    "print('Kruskal-Wallis H Test:')\n",
    "print(f'Statistic: {stat}, p-value: {p}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformation and ANOVA Results\n",
    "\n",
    "To address the normality issue, we applied a logarithmic transformation to the citation data and then performed an ANOVA test. The results are as follows:\n",
    "\n",
    "- **Sum of Squares (C(dominant_topic))**: 128.127529\n",
    "- **Degrees of Freedom (C(dominant_topic))**: 19\n",
    "- **F-statistic**: 3.075667\n",
    "- **p-value**: 0.000007\n",
    "\n",
    "The very low p-value (< 0.05) indicates a statistically significant difference in the mean citation counts among the different dominant topic groups.\n",
    "\n",
    "### Kruskal-Wallis H Test Results\n",
    "\n",
    "Given the initial non-normal distribution of the citation data, we also performed a Kruskal-Wallis H test, a non-parametric alternative to ANOVA. The results are as follows:\n",
    "\n",
    "- **Kruskal-Wallis H Statistic**: 59.060234952429\n",
    "- **p-value**: 5.444105286519811e-06\n",
    "\n",
    "The extremely low p-value (< 0.05) from the Kruskal-Wallis test further confirms a significant difference in citation counts among the different dominant topic groups.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Both the ANOVA test on the log-transformed data and the Kruskal-Wallis H test on the original data provide strong evidence that the mean citation counts differ significantly across the 20 groups categorized by dominant topics.\n",
    "\n",
    "- The ANOVA test indicates significant differences with an F-statistic of 3.075667 and a p-value of 0.000007.\n",
    "- The Kruskal-Wallis H test supports this finding with a test statistic of 59.060234952429 and a p-value of 5.444105286519811e-06.\n",
    "\n",
    "These results suggest that dominant topics play a significant role in the citation counts of the papers, and the differences in citations among these groups are statistically significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having established that the mean citation counts among these groups are statistically significant, the next step is to use regression techniques for potential prediction. By applying regression analysis, we can model the relationship between dominant topics and citation counts, allowing us to predict citation counts based on the identified topics.\n",
    "\n",
    "### Single Linear Regression Model\n",
    "\n",
    "We begin with a single linear regression model where the number of citations serves as the dependent variable, and the topic loadings are the independent variables. This approach helps us understand how variations in the presence and strength of specific topics influence the number of citations an article receives.\n",
    "\n",
    "The single linear regression model can be represented as:\n",
    "\n",
    "𝑌=\n",
    "𝛽\n",
    "0\n",
    "+\n",
    "𝛽\n",
    "1\n",
    "𝑋\n",
    "1\n",
    "+\n",
    "𝜖\n",
    "\n",
    "Where:\n",
    "- 𝑌 is the dependent variable representing the number of citations.\n",
    "- 𝛽0 is the intercept.\n",
    "- 𝛽1 is the coefficient for the independent variable 𝑋1, which represents the topic loadings.\n",
    "- 𝜖 is the error term.\n",
    "\n",
    "By fitting this model to our data, we aim to estimate the coefficients 𝛽0 and 𝛽1, which will provide insights into the strength and direction of the relationship between dominant topics and citation counts. \n",
    "\n",
    "### Model Evaluation\n",
    "\n",
    "To assess the goodness-of-fit of our model, we will use metrics such as the R-squared value. The R-squared value indicates how well our independent variable (topic loadings) explains the variability in the dependent variable (citation counts). A higher R-squared value implies a better fit of the model to the data.\n",
    "\n",
    "### Future Steps\n",
    "\n",
    "This initial application of single linear regression sets the foundation for more complex models, such as multiple linear regression, which can incorporate multiple independent variables (various topic loadings) simultaneously. By progressively refining our models, we aim to enhance the accuracy and predictive power of our analysis, ultimately contributing to a deeper understanding of the factors driving citation counts in academic literature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:             n_citation   R-squared:                       0.004\n",
      "Model:                            OLS   Adj. R-squared:                  0.001\n",
      "Method:                 Least Squares   F-statistic:                     1.465\n",
      "Date:                Sun, 28 Jul 2024   Prob (F-statistic):             0.0870\n",
      "Time:                        00:02:24   Log-Likelihood:                -45627.\n",
      "No. Observations:                7073   AIC:                         9.129e+04\n",
      "Df Residuals:                    7053   BIC:                         9.143e+04\n",
      "Df Model:                          19                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         21.6275      8.888      2.433      0.015       4.204      39.051\n",
      "1             39.0745     11.444      3.414      0.001      16.640      61.509\n",
      "2             15.8671     10.995      1.443      0.149      -5.687      37.421\n",
      "3              4.1478     11.439      0.363      0.717     -18.277      26.572\n",
      "4             15.9891     11.921      1.341      0.180      -7.381      39.359\n",
      "5              2.1499     12.126      0.177      0.859     -21.621      25.921\n",
      "6              6.8107     11.485      0.593      0.553     -15.704      29.325\n",
      "7              8.4011     20.380      0.412      0.680     -31.549      48.351\n",
      "8              9.7373     11.866      0.821      0.412     -13.523      32.998\n",
      "9              6.6784     13.089      0.510      0.610     -18.981      32.337\n",
      "10             0.8782     11.132      0.079      0.937     -20.945      22.701\n",
      "11            11.7881     11.760      1.002      0.316     -11.266      34.842\n",
      "12             7.3220     11.900      0.615      0.538     -16.006      30.650\n",
      "13             1.3891     11.311      0.123      0.902     -20.785      23.563\n",
      "14             3.8974     12.167      0.320      0.749     -19.954      27.749\n",
      "15            12.8479     13.247      0.970      0.332     -13.121      38.817\n",
      "16             1.7196     12.409      0.139      0.890     -22.606      26.045\n",
      "17            -0.0567     13.418     -0.004      0.997     -26.360      26.247\n",
      "18            -2.6999     13.621     -0.198      0.843     -29.401      24.001\n",
      "19            11.6971     12.498      0.936      0.349     -12.802      36.196\n",
      "==============================================================================\n",
      "Omnibus:                    21106.555   Durbin-Watson:                   2.013\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):       1954596979.506\n",
      "Skew:                          42.704   Prob(JB):                         0.00\n",
      "Kurtosis:                    2576.910   Cond. No.                         22.9\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "Mean Squared Error: 7408.406634420357\n",
      "R-squared: 0.0010269514943028746\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Assuming 'data' is your DataFrame\n",
    "# Define the independent variables and dependent variable\n",
    "X = pd.get_dummies(data['dominant_topic'], drop_first=True)\n",
    "y = data['n_citation']\n",
    "\n",
    "# Split the data into training and testing sets using the index\n",
    "train_indices, test_indices = train_test_split(data.index, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create training and testing sets\n",
    "X_train, X_test = X.loc[train_indices], X.loc[test_indices]\n",
    "y_train, y_test = y.loc[train_indices], y.loc[test_indices]\n",
    "\n",
    "# Add a constant term to the independent variables\n",
    "X_train = sm.add_constant(X_train)\n",
    "X_test = sm.add_constant(X_test)\n",
    "\n",
    "# Fit the linear regression model on the training set\n",
    "model = sm.OLS(y_train, X_train).fit()\n",
    "\n",
    "# Print the model summary\n",
    "print(model.summary())\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'R-squared: {r2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The OLS regression model was fitted to predict the number of citations (`n_citation`). The R-squared value is very low at 0.002, indicating that the model explains only 0.2% of the variance in the dependent variable. The F-statistic is 1.147 with a p-value of 0.295, suggesting that the overall model is not statistically significant.\n",
    "\n",
    "### Key Coefficients\n",
    "- **Intercept**: 35.1896 (p < 0.001), representing the baseline number of citations.\n",
    "- **Variable 3**: 16.5802 (p = 0.044), one of the few statistically significant predictors.\n",
    "\n",
    "### Summary\n",
    "Overall, the model demonstrates poor explanatory power, as many predictors do not significantly contribute to explaining the variation in citation counts.\n",
    "\n",
    "## Regression with all topic loadings. \n",
    "Next, we perform the regression using all topic loadings instead of only the domiant topic. \n",
    "The result is still not satisfisfactory. We know already from preivous steps that the citation is not normal distributed. The linear relationship might not exisit. As such, we will furhter perfrom random foreast and deep learning technicals which could potentially take into account any non-linear relationships. Given the size of the avaiable data, they could be better alternatives. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:             n_citation   R-squared:                       0.004\n",
      "Model:                            OLS   Adj. R-squared:                  0.001\n",
      "Method:                 Least Squares   F-statistic:                     1.306\n",
      "Date:                Sat, 27 Jul 2024   Prob (F-statistic):              0.162\n",
      "Time:                        20:06:57   Log-Likelihood:                -45627.\n",
      "No. Observations:                7073   AIC:                         9.130e+04\n",
      "Df Residuals:                    7052   BIC:                         9.144e+04\n",
      "Df Model:                          20                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const        443.7378    347.953      1.275      0.202    -238.355    1125.830\n",
      "topic0      -433.8203    350.199     -1.239      0.215   -1120.315     252.675\n",
      "topic1      -370.4552    349.869     -1.059      0.290   -1056.303     315.393\n",
      "topic2      -411.5437    350.180     -1.175      0.240   -1098.001     274.914\n",
      "topic3      -413.4259    349.597     -1.183      0.237   -1098.741     271.890\n",
      "topic4      -398.7321    349.863     -1.140      0.254   -1084.568     287.104\n",
      "topic5      -414.2619    350.021     -1.184      0.237   -1100.409     271.885\n",
      "topic6      -414.9573    349.811     -1.186      0.236   -1100.692     270.777\n",
      "topic7      -416.4698    349.825     -1.191      0.234   -1102.231     269.292\n",
      "topic8      -422.4423    349.721     -1.208      0.227   -1108.001     263.116\n",
      "topic9      -430.5840    349.593     -1.232      0.218   -1115.891     254.723\n",
      "topic10     -422.2571    349.542     -1.208      0.227   -1107.465     262.951\n",
      "topic11     -391.6845    350.203     -1.118      0.263   -1078.188     294.819\n",
      "topic12     -384.0791    350.227     -1.097      0.273   -1070.629     302.470\n",
      "topic13     -443.6119    349.517     -1.269      0.204   -1128.770     241.547\n",
      "topic14     -410.2707    350.073     -1.172      0.241   -1096.519     275.978\n",
      "topic15     -424.9817    348.511     -1.219      0.223   -1108.168     258.205\n",
      "topic16     -445.3484    349.725     -1.273      0.203   -1130.914     240.217\n",
      "topic17     -446.9953    349.308     -1.280      0.201   -1131.743     237.752\n",
      "topic18     -431.1474    350.248     -1.231      0.218   -1117.739     255.444\n",
      "topic19     -399.3818    349.940     -1.141      0.254   -1085.370     286.606\n",
      "==============================================================================\n",
      "Omnibus:                    21129.640   Durbin-Watson:                   2.014\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):       1971348691.821\n",
      "Skew:                          42.833   Prob(JB):                         0.00\n",
      "Kurtosis:                    2587.920   Cond. No.                         900.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "Mean Squared Error: 7401.502551126926\n",
      "R-squared: 0.0019579199839010464\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Assuming 'data' is your DataFrame\n",
    "# Define the dependent variable\n",
    "y = data['n_citation']\n",
    "\n",
    "# Define the independent variables\n",
    "X = data[['topic0', 'topic1', 'topic2', 'topic3', 'topic4', 'topic5', 'topic6', 'topic7', 'topic8', 'topic9', 'topic10', 'topic11', 'topic12', 'topic13', 'topic14', 'topic15', 'topic16', 'topic17', 'topic18', 'topic19']]\n",
    "\n",
    "\n",
    "\n",
    "# Create training and testing sets\n",
    "X_train, X_test = X.loc[train_indices], X.loc[test_indices]\n",
    "y_train, y_test = y.loc[train_indices], y.loc[test_indices]\n",
    "\n",
    "# Add a constant term to the independent variables\n",
    "X_train = sm.add_constant(X_train)\n",
    "X_test = sm.add_constant(X_test)\n",
    "\n",
    "# Fit the linear regression model on the training set\n",
    "model = sm.OLS(y_train, X_train).fit()\n",
    "\n",
    "# Print the model summary\n",
    "print(model.summary())\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'R-squared: {r2}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 8030.670115479093\n",
      "R-squared: -0.0828810299816154\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Assuming 'data' is your DataFrame\n",
    "# Define the dependent variable\n",
    "y = data['n_citation']\n",
    "\n",
    "# Define the independent variables\n",
    "X = data[['topic0', 'topic1', 'topic2', 'topic3', 'topic4', 'topic5', 'topic6', 'topic7', 'topic8', 'topic9', 'topic10', 'topic11', 'topic12', 'topic13', 'topic14', 'topic15', 'topic16', 'topic17', 'topic18', 'topic19']]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test = X.loc[train_indices], X.loc[test_indices]\n",
    "y_train, y_test = y.loc[train_indices], y.loc[test_indices]\n",
    "\n",
    "# Initialize the Random Forest Regressor\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple forward neural network was trained to create a model capable of predicting the number of citations based on topic loadings. However, the quality of the model is not satisfactory. This outcome further demonstrates that topic loadings cannot be reliably used for citation prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "177/177 [==============================] - 0s 2ms/step - loss: 28180.6602 - val_loss: 7242.9985\n",
      "Epoch 2/100\n",
      "177/177 [==============================] - 0s 781us/step - loss: 27763.1992 - val_loss: 7222.3052\n",
      "Epoch 3/100\n",
      "177/177 [==============================] - 0s 729us/step - loss: 27696.2949 - val_loss: 7226.3584\n",
      "Epoch 4/100\n",
      "177/177 [==============================] - 0s 862us/step - loss: 27667.1758 - val_loss: 7222.8691\n",
      "Epoch 5/100\n",
      "177/177 [==============================] - 0s 782us/step - loss: 27643.6445 - val_loss: 7262.4087\n",
      "Epoch 6/100\n",
      "177/177 [==============================] - 0s 746us/step - loss: 27625.8945 - val_loss: 7240.4990\n",
      "Epoch 7/100\n",
      "177/177 [==============================] - 0s 746us/step - loss: 27617.3496 - val_loss: 7244.6523\n",
      "Epoch 8/100\n",
      "177/177 [==============================] - 0s 795us/step - loss: 27608.2578 - val_loss: 7243.2686\n",
      "Epoch 9/100\n",
      "177/177 [==============================] - 0s 1ms/step - loss: 27593.0625 - val_loss: 7212.1860\n",
      "Epoch 10/100\n",
      "177/177 [==============================] - 0s 724us/step - loss: 27580.4316 - val_loss: 7221.6919\n",
      "Epoch 11/100\n",
      "177/177 [==============================] - 0s 639us/step - loss: 27576.4414 - val_loss: 7221.1196\n",
      "Epoch 12/100\n",
      "177/177 [==============================] - 0s 782us/step - loss: 27567.3164 - val_loss: 7239.3267\n",
      "Epoch 13/100\n",
      "177/177 [==============================] - 0s 720us/step - loss: 27567.9512 - val_loss: 7250.5200\n",
      "Epoch 14/100\n",
      "177/177 [==============================] - 0s 715us/step - loss: 27558.7109 - val_loss: 7221.1094\n",
      "Epoch 15/100\n",
      "177/177 [==============================] - 0s 706us/step - loss: 27547.5430 - val_loss: 7262.9878\n",
      "Epoch 16/100\n",
      "177/177 [==============================] - 0s 882us/step - loss: 27537.5527 - val_loss: 7226.0269\n",
      "Epoch 17/100\n",
      "177/177 [==============================] - 0s 681us/step - loss: 27545.7891 - val_loss: 7214.9146\n",
      "Epoch 18/100\n",
      "177/177 [==============================] - 0s 640us/step - loss: 27527.3262 - val_loss: 7221.8677\n",
      "Epoch 19/100\n",
      "177/177 [==============================] - 0s 749us/step - loss: 27521.6211 - val_loss: 7230.9395\n",
      "Epoch 20/100\n",
      "177/177 [==============================] - 0s 660us/step - loss: 27513.6719 - val_loss: 7253.9648\n",
      "Epoch 21/100\n",
      "177/177 [==============================] - 0s 659us/step - loss: 27490.9219 - val_loss: 7262.7222\n",
      "Epoch 22/100\n",
      "177/177 [==============================] - 0s 749us/step - loss: 27491.4785 - val_loss: 7229.2339\n",
      "Epoch 23/100\n",
      "177/177 [==============================] - 0s 792us/step - loss: 27468.8125 - val_loss: 7235.5161\n",
      "Epoch 24/100\n",
      "177/177 [==============================] - 0s 661us/step - loss: 27455.3633 - val_loss: 7215.7378\n",
      "Epoch 25/100\n",
      "177/177 [==============================] - 0s 754us/step - loss: 27456.3418 - val_loss: 7217.7754\n",
      "Epoch 26/100\n",
      "177/177 [==============================] - 0s 661us/step - loss: 27424.9551 - val_loss: 7266.7979\n",
      "Epoch 27/100\n",
      "177/177 [==============================] - 0s 695us/step - loss: 27411.4668 - val_loss: 7247.2148\n",
      "Epoch 28/100\n",
      "177/177 [==============================] - 0s 841us/step - loss: 27394.9121 - val_loss: 7303.7720\n",
      "Epoch 29/100\n",
      "177/177 [==============================] - 0s 747us/step - loss: 27380.7227 - val_loss: 7229.5132\n",
      "Epoch 30/100\n",
      "177/177 [==============================] - 0s 657us/step - loss: 27357.9551 - val_loss: 7238.1885\n",
      "Epoch 31/100\n",
      "177/177 [==============================] - 0s 660us/step - loss: 27350.4219 - val_loss: 7228.8628\n",
      "Epoch 32/100\n",
      "177/177 [==============================] - 0s 708us/step - loss: 27330.8613 - val_loss: 7308.2891\n",
      "Epoch 33/100\n",
      "177/177 [==============================] - 0s 707us/step - loss: 27302.0371 - val_loss: 7288.0898\n",
      "Epoch 34/100\n",
      "177/177 [==============================] - 0s 691us/step - loss: 27303.5254 - val_loss: 7271.7344\n",
      "Epoch 35/100\n",
      "177/177 [==============================] - 0s 629us/step - loss: 27273.3965 - val_loss: 7283.9570\n",
      "Epoch 36/100\n",
      "177/177 [==============================] - 0s 755us/step - loss: 27258.9102 - val_loss: 7246.9019\n",
      "Epoch 37/100\n",
      "177/177 [==============================] - 0s 943us/step - loss: 27223.7852 - val_loss: 7240.4531\n",
      "Epoch 38/100\n",
      "177/177 [==============================] - 0s 770us/step - loss: 27194.5176 - val_loss: 7345.5732\n",
      "Epoch 39/100\n",
      "177/177 [==============================] - 0s 644us/step - loss: 27192.0020 - val_loss: 7305.1421\n",
      "Epoch 40/100\n",
      "177/177 [==============================] - 0s 783us/step - loss: 27176.3086 - val_loss: 7275.7583\n",
      "Epoch 41/100\n",
      "177/177 [==============================] - 0s 631us/step - loss: 27134.7266 - val_loss: 7345.3916\n",
      "Epoch 42/100\n",
      "177/177 [==============================] - 0s 1ms/step - loss: 27127.8613 - val_loss: 7286.1567\n",
      "Epoch 43/100\n",
      "177/177 [==============================] - 0s 943us/step - loss: 27094.6973 - val_loss: 7300.0522\n",
      "Epoch 44/100\n",
      "177/177 [==============================] - 0s 666us/step - loss: 27081.5156 - val_loss: 7309.8843\n",
      "Epoch 45/100\n",
      "177/177 [==============================] - 0s 752us/step - loss: 27054.7676 - val_loss: 7327.1724\n",
      "Epoch 46/100\n",
      "177/177 [==============================] - 0s 668us/step - loss: 27029.2461 - val_loss: 7392.7720\n",
      "Epoch 47/100\n",
      "177/177 [==============================] - 0s 720us/step - loss: 27013.0430 - val_loss: 7371.0723\n",
      "Epoch 48/100\n",
      "177/177 [==============================] - 0s 855us/step - loss: 26988.9434 - val_loss: 7360.6475\n",
      "Epoch 49/100\n",
      "177/177 [==============================] - 0s 771us/step - loss: 26950.9414 - val_loss: 7369.0078\n",
      "Epoch 50/100\n",
      "177/177 [==============================] - 0s 644us/step - loss: 26945.8672 - val_loss: 7375.8574\n",
      "Epoch 51/100\n",
      "177/177 [==============================] - 0s 785us/step - loss: 26912.6230 - val_loss: 7381.7754\n",
      "Epoch 52/100\n",
      "177/177 [==============================] - 0s 644us/step - loss: 26888.8457 - val_loss: 7386.8496\n",
      "Epoch 53/100\n",
      "177/177 [==============================] - 0s 692us/step - loss: 26866.7598 - val_loss: 7490.7788\n",
      "Epoch 54/100\n",
      "177/177 [==============================] - 0s 932us/step - loss: 26829.4414 - val_loss: 7415.6558\n",
      "Epoch 55/100\n",
      "177/177 [==============================] - 0s 771us/step - loss: 26817.0117 - val_loss: 7623.9653\n",
      "Epoch 56/100\n",
      "177/177 [==============================] - 0s 830us/step - loss: 26802.7793 - val_loss: 7584.5786\n",
      "Epoch 57/100\n",
      "177/177 [==============================] - 0s 1ms/step - loss: 26762.3926 - val_loss: 7513.2671\n",
      "Epoch 58/100\n",
      "177/177 [==============================] - 0s 860us/step - loss: 26734.5859 - val_loss: 7500.1055\n",
      "Epoch 59/100\n",
      "177/177 [==============================] - 0s 943us/step - loss: 26698.6191 - val_loss: 7570.5054\n",
      "Epoch 60/100\n",
      "177/177 [==============================] - 0s 942us/step - loss: 26707.3516 - val_loss: 7537.3335\n",
      "Epoch 61/100\n",
      "177/177 [==============================] - 0s 961us/step - loss: 26651.0684 - val_loss: 7596.8989\n",
      "Epoch 62/100\n",
      "177/177 [==============================] - 0s 801us/step - loss: 26612.5352 - val_loss: 7517.8691\n",
      "Epoch 63/100\n",
      "177/177 [==============================] - 0s 619us/step - loss: 26608.6992 - val_loss: 7572.3716\n",
      "Epoch 64/100\n",
      "177/177 [==============================] - 0s 713us/step - loss: 26591.6465 - val_loss: 7544.6299\n",
      "Epoch 65/100\n",
      "177/177 [==============================] - 0s 764us/step - loss: 26563.6797 - val_loss: 7656.3887\n",
      "Epoch 66/100\n",
      "177/177 [==============================] - 0s 751us/step - loss: 26532.4688 - val_loss: 7659.3853\n",
      "Epoch 67/100\n",
      "177/177 [==============================] - 0s 849us/step - loss: 26522.2090 - val_loss: 7592.2729\n",
      "Epoch 68/100\n",
      "177/177 [==============================] - 0s 770us/step - loss: 26481.4141 - val_loss: 7603.8291\n",
      "Epoch 69/100\n",
      "177/177 [==============================] - 0s 849us/step - loss: 26466.4316 - val_loss: 7735.6069\n",
      "Epoch 70/100\n",
      "177/177 [==============================] - 0s 855us/step - loss: 26426.8164 - val_loss: 7605.5703\n",
      "Epoch 71/100\n",
      "177/177 [==============================] - 0s 920us/step - loss: 26421.6016 - val_loss: 7650.2949\n",
      "Epoch 72/100\n",
      "177/177 [==============================] - 0s 953us/step - loss: 26370.7266 - val_loss: 7681.5986\n",
      "Epoch 73/100\n",
      "177/177 [==============================] - 0s 766us/step - loss: 26344.9141 - val_loss: 7745.1660\n",
      "Epoch 74/100\n",
      "177/177 [==============================] - 0s 751us/step - loss: 26325.0898 - val_loss: 8037.4551\n",
      "Epoch 75/100\n",
      "177/177 [==============================] - 0s 727us/step - loss: 26334.4004 - val_loss: 7742.0288\n",
      "Epoch 76/100\n",
      "177/177 [==============================] - 0s 574us/step - loss: 26259.1562 - val_loss: 7722.8296\n",
      "Epoch 77/100\n",
      "177/177 [==============================] - 0s 657us/step - loss: 26229.5703 - val_loss: 7936.4404\n",
      "Epoch 78/100\n",
      "177/177 [==============================] - 0s 668us/step - loss: 26238.5430 - val_loss: 7765.5596\n",
      "Epoch 79/100\n",
      "177/177 [==============================] - 0s 639us/step - loss: 26196.7363 - val_loss: 7774.0142\n",
      "Epoch 80/100\n",
      "177/177 [==============================] - 0s 660us/step - loss: 26145.2637 - val_loss: 7758.4150\n",
      "Epoch 81/100\n",
      "177/177 [==============================] - 0s 588us/step - loss: 26147.3008 - val_loss: 7782.3022\n",
      "Epoch 82/100\n",
      "177/177 [==============================] - 0s 659us/step - loss: 26122.5742 - val_loss: 7804.5620\n",
      "Epoch 83/100\n",
      "177/177 [==============================] - 0s 659us/step - loss: 26095.3574 - val_loss: 7812.4609\n",
      "Epoch 84/100\n",
      "177/177 [==============================] - 0s 750us/step - loss: 26081.2891 - val_loss: 7820.4663\n",
      "Epoch 85/100\n",
      "177/177 [==============================] - 0s 740us/step - loss: 26030.6426 - val_loss: 7911.6670\n",
      "Epoch 86/100\n",
      "177/177 [==============================] - 0s 786us/step - loss: 26006.6973 - val_loss: 7835.7505\n",
      "Epoch 87/100\n",
      "177/177 [==============================] - 0s 629us/step - loss: 25986.1523 - val_loss: 7920.2544\n",
      "Epoch 88/100\n",
      "177/177 [==============================] - 0s 660us/step - loss: 25964.1055 - val_loss: 7863.3950\n",
      "Epoch 89/100\n",
      "177/177 [==============================] - 0s 675us/step - loss: 25906.3691 - val_loss: 7878.2720\n",
      "Epoch 90/100\n",
      "177/177 [==============================] - 0s 650us/step - loss: 25923.0059 - val_loss: 7878.7207\n",
      "Epoch 91/100\n",
      "177/177 [==============================] - 0s 656us/step - loss: 25897.4336 - val_loss: 7993.2451\n",
      "Epoch 92/100\n",
      "177/177 [==============================] - 0s 684us/step - loss: 25823.4336 - val_loss: 7943.0093\n",
      "Epoch 93/100\n",
      "177/177 [==============================] - 0s 655us/step - loss: 25847.7852 - val_loss: 7961.9624\n",
      "Epoch 94/100\n",
      "177/177 [==============================] - 0s 735us/step - loss: 25801.0430 - val_loss: 7989.4839\n",
      "Epoch 95/100\n",
      "177/177 [==============================] - 0s 801us/step - loss: 25782.6680 - val_loss: 8156.3281\n",
      "Epoch 96/100\n",
      "177/177 [==============================] - 0s 902us/step - loss: 25731.9844 - val_loss: 7975.0332\n",
      "Epoch 97/100\n",
      "177/177 [==============================] - 0s 748us/step - loss: 25688.9648 - val_loss: 8111.9277\n",
      "Epoch 98/100\n",
      "177/177 [==============================] - 0s 691us/step - loss: 25709.5195 - val_loss: 7985.5151\n",
      "Epoch 99/100\n",
      "177/177 [==============================] - 0s 761us/step - loss: 25687.3242 - val_loss: 8035.9727\n",
      "Epoch 100/100\n",
      "177/177 [==============================] - 0s 718us/step - loss: 25633.5391 - val_loss: 8032.7422\n",
      "Mean Squared Error: 8180.914196290419\n",
      "R-squared: -0.10314041838109822\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Define the dependent variable\n",
    "y = data['n_citation']\n",
    "\n",
    "# Define the independent variables\n",
    "X = data[['topic0', 'topic1', 'topic2', 'topic3', 'topic4', 'topic5', 'topic6', 'topic7', 'topic8', 'topic9', 'topic10', 'topic11', 'topic12', 'topic13', 'topic14', 'topic15', 'topic16', 'topic17', 'topic18', 'topic19']]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test = X.loc[train_indices], X.loc[test_indices]\n",
    "y_train, y_test = y.loc[train_indices], y.loc[test_indices]\n",
    "\n",
    "\n",
    "# Normalize the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Build the neural network model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=100, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test).flatten()\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared: {r2}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
