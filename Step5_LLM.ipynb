{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final section of this thesis seeks to explore the application of large language models (LLMs) in the domain of topic modeling. Given the inherent token limitations of LLMs, their direct application to extensive corpora for comprehensive topic extraction presents notable challenges (Devlin et al., 2018). These models are not ideally suited to process large datasets in a single iteration, which complicates their ability to distill and analyze the embedded topics within such datasets. Furthermore, LLMs typically do not have built-in quantitative evaluation metrics that can directly assess the quality and relevance of the topics generated, which poses additional hurdles for their use in systematic topic analysis.\n",
    "\n",
    "However, LLMs hold significant potential in augmenting traditional topic modeling techniques such as Latent Dirichlet Allocation (LDA) and Non-negative Matrix Factorization (NMF). Both LDA and NMF are capable of identifying topics represented by clusters of keywords and their corresponding weights (Blei et al., 2003; Lee & Seung, 1999). However, the interpretation of these topics often remains ambiguous and subject to individual interpretation, as the mere presentation of keywords and weights does not necessarily convey the nuanced context or thematic depth of the topics.\n",
    "\n",
    "This is where LLMs can provide substantial value. By leveraging their advanced natural language processing capabilities, LLMs can be employed to interpret and summarize the topics identified by traditional methods (Raffel et al., 2019). Specifically, LLMs can analyze the clusters of keywords and their weights, synthesizing this information into a more coherent and contextually enriched summary. This process not only enhances the clarity of the topic representation but also provides a narrative that can be more readily understood and utilized by humans.\n",
    "\n",
    "Employing LLMs in this capacity could transform topic modeling from a largely quantitative exercise into a more qualitative and interpretative process. This integration could lead to more precise topic identification, better alignment with human cognitive processes, and potentially, more actionable insights for researchers and practitioners in various fields. Thus, while LLMs face certain limitations in direct application to large-scale topic modeling, their strength lies in their ability to refine and elucidate the outputs of traditional topic modeling techniques, thereby adding a layer of depth and usability that purely algorithmic approaches may lack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "\n",
    "\n",
    "lda_model = models.LdaModel.load(\"ProcessedData/ldamodel\")  # Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "Abstract = pd.read_csv('ProcessedData/abstract_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fluorescence in situ hybridization (FISH) is a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mining association rules is an important issue...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>With the increasing concern about global envir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Automatic summarization evaluation is very imp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>As the number of pages on the web is permanent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8837</th>\n",
       "      <td>This article has been retracted at the request...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8838</th>\n",
       "      <td>The unfolding technique is an efficient tool t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8839</th>\n",
       "      <td>Although potential benefits are frequently men...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8840</th>\n",
       "      <td>Architectural Design Decisions (ADD) form a ke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8841</th>\n",
       "      <td>This paper describes a novel shape-matching ap...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8842 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Abstract\n",
       "0     Fluorescence in situ hybridization (FISH) is a...\n",
       "1     Mining association rules is an important issue...\n",
       "2     With the increasing concern about global envir...\n",
       "3     Automatic summarization evaluation is very imp...\n",
       "4     As the number of pages on the web is permanent...\n",
       "...                                                 ...\n",
       "8837  This article has been retracted at the request...\n",
       "8838  The unfolding technique is an efficient tool t...\n",
       "8839  Although potential benefits are frequently men...\n",
       "8840  Architectural Design Decisions (ADD) form a ke...\n",
       "8841  This paper describes a novel shape-matching ap...\n",
       "\n",
       "[8842 rows x 1 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All topics saved to 'ProcessedData/all_topics_lda.csv'\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "num_topics = 20\n",
    "num_words = 50\n",
    "\n",
    "# Collect all topics with their words and probabilities\n",
    "all_topics = []\n",
    "for topic_id in range(num_topics):\n",
    "    # Extract words and probabilities for the topic\n",
    "    topic_words = lda_model.show_topic(topic_id, num_words)\n",
    "    # Create a string representation of the topic\n",
    "    topic_string = \", \".join([f\"{word} ({prob:.4f})\" for word, prob in topic_words])\n",
    "    all_topics.append([topic_string])\n",
    "\n",
    "# Convert list to DataFrame\n",
    "topics_df = pd.DataFrame(all_topics, columns=['Topic Words'])\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "output_file = 'ProcessedData/all_topics_lda.csv'\n",
    "topics_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"All topics saved to '{output_file}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "write a python function, the input is the paper number, output is the abstract, dominant topics and topic loadings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('study', 0.00799858),\n",
       " ('network', 0.0069343806),\n",
       " ('their', 0.006873849),\n",
       " ('research', 0.00678384),\n",
       " (\"'\", 0.006085316),\n",
       " ('new', 0.005669204),\n",
       " ('technology', 0.005425469),\n",
       " ('at', 0.0048812977),\n",
       " ('knowledge', 0.0047922987),\n",
       " ('different', 0.0045970464),\n",
       " ('information', 0.004454832),\n",
       " ('interaction', 0.00424667),\n",
       " ('process', 0.004090605),\n",
       " ('student', 0.0039881305),\n",
       " ('\"', 0.0038622585),\n",
       " ('work', 0.0038196004),\n",
       " ('between', 0.0038100283),\n",
       " ('or', 0.0037261343),\n",
       " ('they', 0.0036489153),\n",
       " ('design', 0.00362047),\n",
       " ('how', 0.0035869998),\n",
       " ('learning', 0.003555055),\n",
       " ('activity', 0.0033634857),\n",
       " ('more', 0.0033417395),\n",
       " ('provide', 0.0033188546),\n",
       " ('develop', 0.0032589743),\n",
       " ('support', 0.0032436778),\n",
       " (\"'s\", 0.0031785183),\n",
       " ('datum', 0.0031522776),\n",
       " ('its', 0.0031339836),\n",
       " ('one', 0.0031283577),\n",
       " ('level', 0.0031226769),\n",
       " ('will', 0.0030208337),\n",
       " ('not', 0.0030019628),\n",
       " ('well', 0.0029758718),\n",
       " ('development', 0.002974455),\n",
       " ('project', 0.002971783),\n",
       " ('human', 0.002889741),\n",
       " ('theory', 0.0028710435),\n",
       " ('through', 0.0028559642),\n",
       " ('analysis', 0.0028504042),\n",
       " ('/', 0.0027000883),\n",
       " ('learn', 0.0026914442),\n",
       " ('individual', 0.002633487),\n",
       " ('course', 0.0026239327),\n",
       " ('two', 0.0026198379),\n",
       " ('make', 0.0026103219),\n",
       " ('focus', 0.002571404),\n",
       " ('gene', 0.0024843258),\n",
       " (':', 0.0024730961)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.show_topic(0, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'ProcessedData/lda_for_regression.csv'  # Replace with your actual file path\n",
    "data = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "n_citation                3.000000\n",
       "x_tsne                  -15.770838\n",
       "y_tsne                   31.155502\n",
       "x_1_topic_probability     0.439834\n",
       "dominant_topic            0.000000\n",
       "topic0                    0.439834\n",
       "topic1                    0.050912\n",
       "topic2                    0.000000\n",
       "topic3                    0.000000\n",
       "topic4                    0.000000\n",
       "topic5                    0.000000\n",
       "topic6                    0.000000\n",
       "topic7                    0.014453\n",
       "topic8                    0.000000\n",
       "topic9                    0.000000\n",
       "topic10                   0.000000\n",
       "topic11                   0.000000\n",
       "topic12                   0.000000\n",
       "topic13                   0.131584\n",
       "topic14                   0.000000\n",
       "topic15                   0.161673\n",
       "topic16                   0.140692\n",
       "topic17                   0.053284\n",
       "topic18                   0.000000\n",
       "topic19                   0.000000\n",
       "Name: 100, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data.iloc[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Neurons of primary sensory cortices are known to have specific responsiveness to elemental features. To express more complex sensory attributes that are embedded in objects or events, the brain must integrate them. This is referred to as feature binding and is reflected in correlated neuronal activity. We investigated how local intracortical circuitry modulates ongoing-spontaneous neuronal activity, which would have a great impact on the processing of subsequent combinatorial input, namely, on the correlating (binding) of relevant features. We simulated a functional, minimal neural network model of primary visual cortex, in which lateral excitatory connections were made in a diffusive manner between cell assemblies that function as orientation columns. A pair of bars oriented at specific angles, expressing a visual corner, was applied to the network. The local intracortical circuitry contributed not only to inducing correlated neuronal activation and thus to binding the paired features but also to making membrane potentials oscillate at firing-subthreshold during an ongoing-spontaneous time period. This led to accelerating the reaction speed of principal cells to the input. If the lateral excitatory connections were selectively (instead of \"diffusively\") made, hyperpolarization in ongoing membrane potential occurred and thus the reaction speed was decelerated. We suggest that the local intracortical circuitry with diffusive connections between cell assemblies might endow the network with an ongoing subthreshold neuronal state, by which it can send the information about combinations of elemental features rapidly to higher cortical stages for their full and precise analyses.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Abstract['Abstract'][100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://chatgpt.com/share/173fd80b-3a02-4e90-93d8-dd0af8c79a3b"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
